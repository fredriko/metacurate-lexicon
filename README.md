# Metacurate Lexicon
## tl;dr
The metacurate lexicon, and the accompanying API, are the results of an investigation into the feasibility 
to deploy a web service that uses a reasonably large set of word embeddings to platform-as-a-service Heroku.

Upcoming features at [metacurate.io](https://metacurate.io) require access to a lexicon of semantically similar multi-word 
terms. Since metacurate.io is hosted on [heroku](https://www.heroku.com/), 
I wanted to find out whether the required semantic lexicon functionality can be deployed to heroku too,
without violating their application size constraints.

The answer is *yes*.


## How to run the web service locally

### Pre-requisites and installation

You will need:

* Python 3.6
* virutalenv 

Optional: If you plan on using retrieve and use the data from metacurate.io, you will also 
need to have a MongoDb instance running (see section *The data* below). 

* Create a virtual environment, let's call it *metacurate-lexicon*
* *pip install -r requirements.txt*


### Running the service

## How to deploy the web service to Heroku

## Background
This is a python/Flask web application that exposes interfaces (a web GUI and a RESTful API) for looking up semantically 
similar (multi-word) terms in a lexicon, as well as the appropriate pre-processing of raw text into sentences and term tokens. The 
word embeddings in the lexicon are generated by the gensim word2vec implementation, and the recognition of multi-word terms is based
on gensim Phraser:s.
 
## The data
The starting point of this endeavour is the data collected by [metacurate.io](https://metacurate.io): at the time of
the this project, there were around 7000 URLs collected specifically related to artificial intelligence, data science, 
natural language processing, deep learning, and all sorts of data-related issues. After pre-processing, the texts
amounted to some 280k sentences (longer than four tokens), and 6,35M words.

Now, that isn't all that much. We clearly need more data to be able to train reliable collocation models, as well
as semantic lexicons.

If you are logged into [webhose.io](https://webhose.io), you have access to a number of 
[free data sets](https://webhose.io/datasets/). The collocations, 
as well as some of the lexicon, are based on the following datasets:

* **English news articles.**
English news articles originated in the US from the top 1,000 (based on the ranking provided by Alexa) news sites,	*499,610 articles*,	Nov 2016
* **Technology news articles.**
English news articles dealing with technology,	*41,476 articles*,	Sep-Oct 2015
* **Random posts from online message boards.**
English posts originating from the US based forums & message boards with over 20 participants in the thread,	*1,806,440 posts*,	Mar 2017
* **Popular News articles.**
English news articles with at least 100 Facebook likes within 3 days of original post,	*170,882 articles*,	Feb-Mar 2017
* **Popular Blog posts.** 
English blog posts with at least 100 Facebook likes within 3 days of original post,	*87,510 posts*,	Feb-Mar 2017

### Making the data useful for training

Download 

## Installation

(If metacurate data: MongoDb required)

## Deploy to heroku

# TODO
* Get and display information about the lexicon, e.g., how many entries, common multi-word expressions, etc.
* Add morphology to lookup results in API
